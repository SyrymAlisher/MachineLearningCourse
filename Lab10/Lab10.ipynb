{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learnining: Lab and HW 10\n",
    "### Homework Tasks:\n",
    "* Plot the error\n",
    "* Model XOR with the help of sigmoid, linear\n",
    "* Add moments rule to learning equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 1\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-k*x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - x**2\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_prime(x):\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.78259327 -0.56963558 -0.56847692]\n",
      " [-0.88564761 -0.83153887 -0.27586914]\n",
      " [ 0.02491959  0.33828785  0.96526953]]\n",
      "[[-0.5729843 ]\n",
      " [-0.64967385]\n",
      " [ 0.25536026]]\n",
      "[0 0] [0.52133207]\n",
      "[0 1] [0.54450588]\n",
      "[1 0] [0.73211536]\n",
      "[1 1] [0.79079991]\n",
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n",
      "[0 0] [8.85832422e-05]\n",
      "[0 1] [0.99424872]\n",
      "[1 0] [0.99271903]\n",
      "[1 1] [3.20940095e-05]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.activation = tanh\n",
    "        self.activation_prime = tanh_prime\n",
    "#         self.activation = linear\n",
    "#         self.activation_prime = linear_prime\n",
    "#         self.activation = sigmoid\n",
    "#         self.activation_prime = sigmoid_prime\n",
    "        \n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        \n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "            print(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        print(r)\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "        \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1):\n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "\n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "\n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "#     X = np.array([[-1, -1],\n",
    "#                   [-1, 1],\n",
    "#                   [1, -1],\n",
    "#                   [1, 1]])\n",
    "#     y = np.array([0, 1, 1, 0])\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))\n",
    "    nn.fit(X, y)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.14261514 -0.70313672 -0.43402857]\n",
      " [ 0.09691952 -0.04696332  0.67863107]\n",
      " [-0.04326058 -0.00326457  0.86867068]]\n",
      "[[-0.22643819]\n",
      " [-0.5602814 ]\n",
      " [ 0.78247399]]\n",
      "[0 0] [0.50551099]\n",
      "[0 1] [0.67114802]\n",
      "[1 0] [0.63585592]\n",
      "[1 1] [0.77708506]\n",
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n",
      "[0 0] [0.50113041]\n",
      "[0 1] [0.49917878]\n",
      "[1 0] [0.50047613]\n",
      "[1 1] [0.4985245]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class XOR:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.activation_linear = linear\n",
    "        self.activation_linear_prime = linear_prime\n",
    "        self.activation_sigmoid = sigmoid\n",
    "        self.activation_sigmoid_prime = sigmoid_prime\n",
    "        \n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        self.moments = []\n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "            self.moments.append(np.zeros(r.shape))\n",
    "            print(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        print(r)\n",
    "        self.weights.append(r)\n",
    "        self.moments.append(np.zeros(r.shape))\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000, gamma=0.1):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "        errors = []\n",
    "        \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "            \n",
    "            for l in range(len(self.weights)-1):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation_linear(dot_value)\n",
    "                    a.append(activation)\n",
    "            a.append(self.activation_sigmoid(np.dot(a[-1], self.weights[-1])))\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            errors.append(error**2)\n",
    "            deltas = [error * self.activation_sigmoid_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1):\n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_linear_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                prev_weight = np.copy(self.weights[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta) + gamma*self.moments[i]\n",
    "                self.moments[i] = self.weights[i]-prev_weight\n",
    "\n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "                \n",
    "        return errors\n",
    "\n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "\n",
    "        for l in range(len(self.weights)-1):\n",
    "            a = self.activation_linear(np.dot(a, self.weights[l]))\n",
    "        return self.activation_sigmoid(np.dot(a, self.weights[-1]))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = XOR([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "#     X = np.array([[-1, -1],\n",
    "#                   [-1, 1],\n",
    "#                   [1, -1],\n",
    "#                   [1, 1]])\n",
    "#     y = np.array([0, 1, 1, 0])\n",
    "    epochs = 100000\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))\n",
    "    errors = nn.fit(X, y, epochs=epochs, learning_rate=0.2, gamma=0.1)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))\n",
    "        \n",
    "    plt.plot(np.arange(epochs), errors)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
